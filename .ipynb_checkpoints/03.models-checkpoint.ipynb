{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5875259a-4d80-4224-8289-159ff0f3134f",
   "metadata": {},
   "source": [
    "## Creating a Transformer\n",
    "\n",
    "The AutoModel class, which is handy when you want to instantiate any model from a checkpoint.\n",
    "\n",
    "The AutoModel class and all of its relatives are actually simple wrappers over the wide variety of models available in the transformers ibrary. It’s a clever wrapper as it can automatically guess the appropriate model architecture for your checkpoint, and then instantiates a model with this architecture.\n",
    "\n",
    "However, if you know the type of model you want to use, you can use the class that defines its architecture directly. Let’s take a look at how this works with a BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d59b711-f58d-431a-8167-cbcf2e7b69df",
   "metadata": {},
   "source": [
    "### Different loading methods\n",
    "Creating a model from the default configuration initializes it with random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d688a90e-611a-4bbb-8a56-8e764877d13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ae21726-0e11-42c9-9223-2510976f6fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f46a36d-a79e-4d16-bb2f-b05502be9c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the config\n",
    "config = BertConfig()\n",
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5918723-27f6-43b0-8644-85d0e7a96a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.configuration_bert.BertConfig"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6696fc0-3783-498d-9a9b-83f1c17d6dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model from the config\n",
    "# Model is randomly initialized!\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5444b77-fb3c-4770-a05c-a3f0c604608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb53b204-06b9-4c2f-bbf9-8f0b56df1829",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"google-bert/bert-base-cased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c3a5f4-f25e-4111-9eea-7b32da1923fb",
   "metadata": {},
   "source": [
    "We can change any part of the configuration using keyword arguments. We can see that this model has 12 hidden layers. We can use 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffbb5b93-d263-4ae9-a9b8-a266b7998b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config_10 = BertConfig.from_pretrained(checkpoint, num_hidden_layers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a909c09-21d4-4c54-bccb-caf59ab36b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_config_10.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "31f9887e-b84d-4a9e-8048-5b944812da9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_10 = BertModel(bert_config_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e0e44244-d43d-4eb7-bf29-b5cfe5f2bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_10.save_pretrained('./saved_models/bert_model_10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "351893d2-65a0-41c7-a2dc-b89ec31e8025",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_bert_10 = BertModel.from_pretrained('./saved_models/bert_model_10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "461a2799-5278-42ea-bf17-8b9818848d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_bert_10.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5015412d-aa41-4a3b-83a5-fd479858c875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "252bcb47-9694-4feb-bad6-fe158fd337d3",
   "metadata": {},
   "source": [
    "The model can be used in this state, but it will output gibberish; it needs to be trained first. We could train the model from scratch on the task at hand, but this would require a long time and a lot of data, and it would have a non-negligible environmental impact. \n",
    "\n",
    "To avoid unnecessary and duplicated effort, it’s imperative to be able to share and reuse models that have already been trained.\n",
    "\n",
    "Loading a Transformer model that is already trained is simple — we can do this using the from_pretrained() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "607122b7-3f09-41af-a40c-be3ae8350b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01783174b08d49d38ac0d89c8bda028e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d4e1aa4dd34fc2be4142507428cf57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"google-bert/bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd634fb0-728d-4354-a6d7-3ebcbc071324",
   "metadata": {},
   "source": [
    "We could replace BertModel with the equivalent AutoModel class as this produces checkpoint-agnostic code.\n",
    "\n",
    "If your code works for one checkpoint, it should work seamlessly with another. This applies even if the architecture is different, as long as the checkpoint was trained for a similar task (for example, a sentiment analysis task).\n",
    "\n",
    "In the code sample above we didn’t use BertConfig, and instead loaded a pretrained model via the bert-base-cased identifier. This is a model checkpoint that was trained by the authors of BERT themselves.\n",
    "\n",
    "This model is now initialized with all the weights of the checkpoint. It can be used directly for inference on the tasks it was trained on, and it can also be fine-tuned on a new task. By training with pretrained weights rather than from scratch, we can quickly achieve good results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf42649-c948-4ba7-bc00-6d239d83c4cc",
   "metadata": {},
   "source": [
    "### Auto Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d865706b-20d3-4770-bf9f-7d0d6e7a61ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff2cdd22-e4d1-461a-9039-b2e809e0afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = AutoConfig.from_pretrained(\"google-bert/bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "99554f04-a905-4fc8-886f-c77d73d80636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.configuration_bert.BertConfig"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "010373e0-5d7b-429c-9f55-cf9e133cc28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"google-bert/bert-base-cased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.48.3\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675bf916-778d-473e-8aef-f3cc3eff8120",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4cfaaf1-f42c-444b-a8ed-3f96cdb87c26",
   "metadata": {},
   "source": [
    "## Saving methods\n",
    "Saving a model is as easy as loading one — we use the save_pretrained() method, which is analogous to the from_pretrained() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c9524e2-e08f-4e1b-819f-f6ac9b04446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('saved_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6499bbe-15ba-49db-9b85-f2e09fff9222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json       model.safetensors\n"
     ]
    }
   ],
   "source": [
    "!ls saved_models/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cc3687-44a7-4198-ab61-4b6cd75522e7",
   "metadata": {},
   "source": [
    "If you take a look at the config.json file, you’ll recognize the attributes necessary to build the model architecture. This file also contains some metadata, such as where the checkpoint originated and what 🤗 Transformers version you were using when you last saved the checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73151e67-62d4-4929-8159-f2d6820288c0",
   "metadata": {},
   "source": [
    "### Loading the pretrained model\n",
    "\n",
    "#### What is SafeTensors?\n",
    "\n",
    "SafeTensors is a format introduced by Hugging Face as an alternative to .pt or .bin files. It provides:\n",
    "\n",
    "- Faster loading: Uses memory mapping for efficient tensor loading.\n",
    "- Security: Prevents arbitrary code execution (unlike pickle-based .pt or .bin formats).\n",
    "- Portability: Works well with Hugging Face models.\n",
    "\n",
    "#### Usage in Hugging Face Models\n",
    "If a model supports SafeTensors, you might see a model.safetensors file when loading or saving a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fa2f195-33ae-423a-86b2-f46d24137954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea4646ad-c802-44ad-81a6-a2e9159f771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = AutoModel.from_pretrained(\"google-bert/bert-base-cased\", use_safetensors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688bdcda-afc6-4e0c-a214-28466769fc23",
   "metadata": {},
   "source": [
    "### Using a Transformer model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1738a59-5570-452c-9195-f55f016e66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "609c293f-7c41-4bc2-96fc-7521f7de996e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6462a97454ec4c3ea66925968d275961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b988018cd74f1a953c91d4ec3d843e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7910ae16734a8bac8ce05f9e40c5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ec60b0f-ec69-4166-83ad-912dfa17c310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101, 8667, 106, 102], [101, 13297, 119, 102], [101, 8835, 106, 102]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sequences = tokenizer(sequences)['input_ids']\n",
    "encoded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e168cb5-3995-4fcf-997e-508a8490210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85487b7c-7748-4ad3-9036-8222d26b2be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  8667,   106,   102],\n",
       "        [  101, 13297,   119,   102],\n",
       "        [  101,  8835,   106,   102]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = torch.tensor(encoded_sequences)\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "844e3267-702b-48d1-b435-6a34385d9e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.6283,  0.2166,  0.5605,  ...,  0.0136,  0.6158, -0.1712],\n",
       "         [ 0.6108, -0.2253,  0.9263,  ..., -0.3028,  0.4500, -0.0714],\n",
       "         [ 0.8040,  0.1809,  0.7076,  ..., -0.0685,  0.4837, -0.0774],\n",
       "         [ 1.3290,  0.2360,  0.4567,  ...,  0.1509,  0.9621, -0.4841]],\n",
       "\n",
       "        [[ 0.3128,  0.1718,  0.2099,  ..., -0.0721,  0.4919, -0.1383],\n",
       "         [ 0.1545, -0.3757,  0.7187,  ..., -0.3130,  0.2822,  0.1883],\n",
       "         [ 0.4123,  0.3721,  0.5484,  ...,  0.0788,  0.5681, -0.2757],\n",
       "         [ 0.8356,  0.3964, -0.4121,  ...,  0.1838,  1.6365, -0.4806]],\n",
       "\n",
       "        [[ 0.5399,  0.2564,  0.2511,  ..., -0.1760,  0.6063, -0.1803],\n",
       "         [ 0.2609, -0.3164,  0.5548,  ..., -0.3439,  0.3909,  0.0900],\n",
       "         [ 0.5161,  0.0721,  0.5606,  ...,  0.0077,  0.3685, -0.2272],\n",
       "         [ 0.6560,  0.8475, -0.1606,  ..., -0.0468,  1.6309, -0.5047]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.7105,  0.4876,  0.9999,  ...,  1.0000, -0.9179,  0.9894],\n",
       "        [-0.7731,  0.5619,  1.0000,  ...,  1.0000, -0.8397,  0.9944],\n",
       "        [-0.7594,  0.5645,  1.0000,  ...,  1.0000, -0.9015,  0.9969]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = load_model(model_inputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75987754-6706-48de-9d79-6d3044316454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 768])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd86218-6230-4c84-b338-e496fd2b6920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24a43ce-a738-4de2-90ec-fbadd9b154f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e82c78-734b-47f0-abf0-169cdfa0da35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91018ec2-874e-472f-9b4d-3e567dc81a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affc0a8f-c107-4035-a5d8-64b485b0eed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
