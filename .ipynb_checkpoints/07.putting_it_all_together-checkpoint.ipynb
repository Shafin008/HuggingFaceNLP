{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e8199bb-8657-4c81-a9a4-c277d27adf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fa3a2c2-8c23-4101-a564-6fadff51a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8cdbb5-b068-4b49-a157-55621f65d8eb",
   "metadata": {},
   "source": [
    "## Tokenizer with single sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "666cb170-29b6-4068-a185-82b699e37ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee227134-9fb0-4e61-88f9-e0123b3bcd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cdf9688-b903-4071-b8ce-995b3786dadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model inputs\n",
    "model_inputs = tokenizer(sequence)\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc117a-3255-4704-a4a2-e75299530dfa",
   "metadata": {},
   "source": [
    "Here, the `model_inputs` variable contains everything that’s necessary for a model to operate well. \n",
    "\n",
    "For `DistilBERT`, that includes the `input IDs` as well as the `attention mask`. Other models that accept additional inputs will also have those output by the `tokenizer` object.\n",
    "\n",
    "As we’ll see in some examples below, this method is very powerful. First, it can tokenize a single sequence as well as multiple sequences at a time, with no change in the API:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b05dd4a-2a6e-415b-bce5-202ad073c63b",
   "metadata": {},
   "source": [
    "## Tokenizer with multiple sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05c84043-22e9-4415-8437-ba30729d40a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \n",
    "    \"So have I!\"\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer(sequences)\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c322d1-c28d-4047-bf7e-199f04a5cfd4",
   "metadata": {},
   "source": [
    "### Padding techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1e76bc-9576-4e06-8468-0b1e45555c75",
   "metadata": {},
   "source": [
    "It can pad according to several objectives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98753c33-92c0-4e00-a107-ca4c544c08d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Will pad the sequences up to the maximum sequence length\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab0bd28d-b5da-44cf-9790-15e3a14e165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will pad the sequences up to the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "# model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "970c4dcd-d09f-459e-8703-b4a9ca75d47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Will pad the sequences up to the specified max length\n",
    "max_length = 8\n",
    "model_inputs = tokenizer(sequences, max_length=max_length, padding=\"max_length\")\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b89b7-cf48-461e-b41e-f4ebdb046f60",
   "metadata": {},
   "source": [
    "### Truncation techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db54667-9a94-4bfe-874c-164814317c24",
   "metadata": {},
   "source": [
    "It can also truncate sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d446e57-e834-4e25-946c-71d1942df1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I've been waiting for a HuggingFace course my whole life.\", 'So have I!']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc2dc461-bfca-45a2-914a-54b25f592eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Will truncate the sequences that are longer than the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, truncation=True)\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbeb5397-7834-4cdd-8744-926c817b7656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Will truncate the sequences that are longer than the specified max length\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation=True)\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fd9f30-f22e-491e-9704-ce4ec471e9ee",
   "metadata": {},
   "source": [
    "### Framework specific tensors\n",
    "The tokenizer object can handle the conversion to specific framework tensors, which can then be directly sent to the model. \n",
    "\n",
    "For example, in the following code sample we are prompting the tokenizer to return tensors from the different frameworks — \"pt\" returns PyTorch tensors, \"tf\" returns TensorFlow tensors, and \"np\" returns NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34f7324a-6d73-4e39-b3ba-17fea23162d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
       "        12172,  2607,  2026,  2878,  2166,  1012,   102],\n",
       "       [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns NumPy arrays\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "946c2022-c9ec-4573-a2d3-b2d3ae67a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns TensorFlow tensors\n",
    "# model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n",
    "# model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e2d6863-7c11-42e5-b159-95f4f2d30cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
       "          2607,  2026,  2878,  2166,  1012,   102],\n",
       "        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns PyTorch tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4867f505-9893-46ef-832b-dc110dc16658",
   "metadata": {},
   "source": [
    "## Special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e82d7ac8-7ece-4c43-9785-f6bed0bffae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0d94a9a-c425-4b98-8407-18c4f91d0e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer(sequence)\n",
    "print(model_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc449e9a-8b8d-4c4e-b1d5-44f103c84b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a98b8b-86f5-4ed7-8196-a93f0a76aafa",
   "metadata": {},
   "source": [
    "One token ID was added at the beginning, and one at the end. Let’s decode the two sequences of IDs above to see what this is about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9feb709-d6cc-4ddb-996c-24136eeb41e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] i've been waiting for a huggingface course my whole life. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model_inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3819f7a7-9f25-4965-a56e-bea16366bb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i've been waiting for a huggingface course my whole life.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090d9bf0-9bbd-45c1-bddd-dd6c1b5ce410",
   "metadata": {},
   "source": [
    "The tokenizer added the special word [CLS] at the beginning and the special word [SEP] at the end. This is because the model was pretrained with those, so to get the same results for inference we need to add them as well. \n",
    "\n",
    "Note that some models don’t add special words, or add different ones; models may also add these special words only at the beginning, or only at the end. \n",
    "\n",
    "In any case, the tokenizer knows which ones are expected and will deal with this for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7d3778-a890-45a0-b0f1-9217d3047212",
   "metadata": {},
   "source": [
    "## From tokenizer to model\n",
    "Now that we’ve seen all the individual steps the tokenizer object uses when applied on texts, let’s see one final time how it can handle multiple sequences (padding!), very long sequences (truncation!), and multiple types of tensors with its main API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ade9458d-a15f-4506-bed2-c0182b974adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d9550ae-2ff1-49e0-8000-4e1f761cf024",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \n",
    "    \"So have I!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a5d76ed-7411-46ea-8fb1-db5e533fcf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "635d5f8f-6c1f-4398-bb90-3a1d6a850dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6af5aa3f-ee08-44e8-bb81-5a028c5e151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e2fb089-c911-412d-a229-5156c0ac45ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]),\n",
      " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "tokenizer_output = tokenizer(\n",
    "    sequences,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "pprint(tokenizer_output, compact=True, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48a887c8-c75a-4d56-9a0c-9f83ec58bd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
       "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**tokenizer_output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ea86839-1dda-4a7d-8444-a4512ed33733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = model(\n",
    "#     tokenizer_output['input_ids'],\n",
    "#     tokenizer_output['attention_mask']\n",
    "# )\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e208f-774e-4e31-bb1b-c6a3f32d6c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b95646f-4165-4145-894f-2c7d70316499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6097f5e-f8bf-425a-97c3-fa66224471f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
