{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3113cc32-54d8-4aae-81db-3c0da08d92ea",
   "metadata": {},
   "source": [
    "We explored the simplest of use cases: doing inference on a single sequence of a small length. However, some questions emerge already:\n",
    "\n",
    "- How do we handle multiple sequences?\n",
    "- How do we handle multiple sequences of different lengths?\n",
    "- Are vocabulary indices the only inputs that allow a model to work well?\n",
    "- Is there such a thing as too long a sequence?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5623a261-7de0-471d-8d9c-6f4ca48d24b0",
   "metadata": {},
   "source": [
    "## Models expect a batch of inputs\n",
    "Previously we saw, sequences get translated into lists of numbers. Let’s convert this list of numbers to a tensor and send it to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08f5a8ab-7af1-4fbb-8ced-59e39292c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6139068-9e45-4f85-8e97-ec4c0fe84b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our model\n",
    "checkpoint = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f24925e-3baa-46da-9c4b-b493b983db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize out tokenizer based on our model\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d772c88a-b36e-4510-be60-3c7608e2ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing our model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "048b69c9-3242-4af1-abef-1fdad7eb88d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95e12c46-eed6-4b2e-83d3-f80bdeefbd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course',\n",
      " 'my', 'whole', 'life', '.']\n"
     ]
    }
   ],
   "source": [
    "# Creating tokens from sequence\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "pprint(tokens, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "202aca91-dd79-4dad-8d82-d3e8e113bec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166,\n",
      " 1012]\n"
     ]
    }
   ],
   "source": [
    "# Converting tokens to numbers(ids)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "pprint(ids, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23f3179c-0878-4198-95cd-4ff64973bd99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9b1d3f4-1b42-4107-bd52-c571a404d8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
       "         2026,  2878,  2166,  1012])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting ids from list to tensors\n",
    "input_ids = torch.tensor(ids)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e3503c0-7ecf-4107-bff4-b4ce05cbd8b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# sending the tensor ids to models\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# it'll give an error\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model(input_ids)\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:977\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    975\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 977\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistilbert(\n\u001b[1;32m    978\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    979\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    980\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m    981\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    982\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    983\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    984\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    985\u001b[0m )\n\u001b[1;32m    986\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    987\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:771\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n\u001b[1;32m    772\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.12/site-packages/transformers/modeling_utils.py:5071\u001b[0m, in \u001b[0;36mPreTrainedModel.warn_if_padding_and_no_attention_mask\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m   5068\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   5070\u001b[0m \u001b[38;5;66;03m# Check only the first and last input IDs to reduce overhead.\u001b[39;00m\n\u001b[0;32m-> 5071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01min\u001b[39;00m input_ids[:, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]]:\n\u001b[1;32m   5072\u001b[0m     warn_string \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   5073\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5074\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/troubleshooting\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5075\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#incorrect-output-when-padding-tokens-arent-masked.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5076\u001b[0m     )\n\u001b[1;32m   5078\u001b[0m     \u001b[38;5;66;03m# If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an\u001b[39;00m\n\u001b[1;32m   5079\u001b[0m     \u001b[38;5;66;03m# attention_mask or not. In this case, we should still show a warning because this is a rare case.\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "# sending the tensor ids to models\n",
    "# it'll give an error\n",
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e5e9de-f606-480c-850c-7392e6c4296f",
   "metadata": {},
   "source": [
    "The problem is that we sent a single sequence to the model, whereas Transformers models expect multiple sentences by default. Here we tried to do everything the tokenizer did behind the scenes when we applied it to a sequence. \n",
    "\n",
    "But if you look closely, you’ll see that the tokenizer didn’t just convert the list of input IDs into a tensor, it added a dimension on top of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea65fb7-2638-4493-a891-c4acf00d2c39",
   "metadata": {},
   "source": [
    "Let’s try again and add a new dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47ed5ef5-9767-443c-9d5a-11eb0cbb4370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, We have to send our model and tensors to device\n",
    "# Otherwise in my case kernel dies\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecafe867-147b-4d8b-af73-b02e63ee416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbf33e3d-9105-49f3-b92e-63664883c8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
       "          2026,  2878,  2166,  1012]], device='mps:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor([ids], device=device)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1a5cbba-4d48-4182-98ee-032493caac11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7276,  2.8789]], device='mps:0', grad_fn=<LinearBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model output\n",
    "output = model(input_ids)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc65e3-c920-46ae-ba19-88b204be3af1",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32382c22-ad9b-4470-bcf6-e526e1966110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "torch.mps.empty_cache()\n",
    "device = torch.device('mps')\n",
    "\n",
    "# Data\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "# Model\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Initializing model and send to device\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "# Initializing tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# tokens\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "# tokens to ids\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Adding a dimension to ids and converting ids from list to tensors\n",
    "# Also sending it to device\n",
    "input_ids = torch.tensor([ids], device=device)\n",
    "\n",
    "# model output\n",
    "output = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f78efeb-2d85-4df1-915f-29ee3b11b52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7276,  2.8789]], device='mps:0', grad_fn=<LinearBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73a931dd-ff2f-40a0-8271-6c6f68a21fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7276,  2.8789]], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits\n",
    "logits = output.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2578de-8182-4910-a34d-819bc902f94f",
   "metadata": {},
   "source": [
    "Batching is the act of sending multiple sentences through the model, all at once. If you only have one sentence, you can just build a batch with a single sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ccd7569c-85cc-49f4-9fea-cd70a28ca82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878,\n",
      "  2166, 1012],\n",
      " [1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878,\n",
      "  2166, 1012]]\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [ids, ids]\n",
    "pprint(batched_ids, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2ec029-2490-4d31-913b-7806d6c4a5dd",
   "metadata": {},
   "source": [
    "This is a batch of two identical sequences!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9647b37-525c-430e-bbc9-65cd05f98543",
   "metadata": {},
   "source": [
    "Convert this batched_ids list into a tensor and pass it through our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7776d467-00f8-45f4-8bd8-9d71703d9cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
       "          2026,  2878,  2166,  1012],\n",
       "        [ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
       "          2026,  2878,  2166,  1012]], device='mps:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_input_ids = torch.tensor(batched_ids, device=device)\n",
    "batched_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "739836bb-ddba-4430-a6a9-98c71df34146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7276,  2.8789],\n",
       "        [-2.7276,  2.8789]], device='mps:0', grad_fn=<LinearBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batched model output\n",
    "batch_output = model(batched_input_ids)\n",
    "batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3ee6d9a-351f-4241-886a-aceb087f26f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7276,  2.8789],\n",
       "        [-2.7276,  2.8789]], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_logits = batch_output.logits\n",
    "batch_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8fad6c-12da-4fc6-985e-9dcdd3a71f30",
   "metadata": {},
   "source": [
    "We obtain the same logits as before (but twice)!\n",
    "\n",
    "Batching allows the model to work when you feed it multiple sentences. Using multiple sequences is just as simple as building a batch with a single sequence. \n",
    "\n",
    "There’s a second issue, though. When you’re trying to batch together two (or more) sentences, they might be of different lengths. \n",
    "\n",
    "If you’ve ever worked with tensors before, you know that they need to be of rectangular shape, so you won’t be able to convert the list of input IDs into a tensor directly. \n",
    "\n",
    "To work around this problem, we usually pad the inputs.\n",
    "\n",
    "## Padding the inputs\n",
    "The following list of lists cannot be converted to a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4facd68-9b90-4512-a971-c268d6a91dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38df40bd-5ba6-45d2-a552-d2b7e6999b6b",
   "metadata": {},
   "source": [
    "In order to work around this, we’ll use padding to make our tensors have a rectangular shape. \n",
    "\n",
    "Padding makes sure all our sentences have the same length by adding a special word called the padding token to the sentences with fewer values. \n",
    "\n",
    "For example, if you have 10 sentences with 10 words and 1 sentence with 20 words, padding will ensure all the sentences have 20 words. \n",
    "\n",
    "In our example, the resulting tensor looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d9c11d5-f38f-4823-a824-5f5ec18ddb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_id = 100\n",
    "\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, padding_id],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b94352-44d3-4bf7-b367-b98060c1f0b1",
   "metadata": {},
   "source": [
    "The padding token ID can be found in `tokenizer.pad_token_id`. \n",
    "\n",
    "Let’s use it and send our two sentences through the model individually and batched together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0f37537-b612-4f4a-8308-5d9b7b5126ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb5a493d-7836-463a-800d-524affec572b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e23ffaba-4572-4682-bbfe-a872dd96cc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we batch the sequence ids\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1dfc7742-6888-44ef-becd-e981d6151e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting sequences and batch to tensors and moving to device\n",
    "sequence1_ids = torch.tensor(sequence1_ids, device=device)\n",
    "sequence2_ids = torch.tensor(sequence2_ids, device=device)\n",
    "batched_ids = torch.tensor(batched_ids, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ab3923c-6326-4c95-9ceb-553ca304ded7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3373, -1.2163]], device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# printing their logits\n",
    "print(model(sequence1_ids).logits)\n",
    "print(model(sequence2_ids).logits)\n",
    "print(model(batched_ids).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a80ace-6f30-41d8-86f3-3fa899a068cd",
   "metadata": {},
   "source": [
    "There’s something wrong with the logits in our batched predictions: the second row should be the same as the logits for the second sentence, but we’ve got completely different values!\n",
    "\n",
    "This is because the key feature of Transformer models is attention layers that `contextualize` each token. These will take into account the padding tokens since they attend to all of the tokens of a sequence. \n",
    "\n",
    "To get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, we need to tell those attention layers to ignore the padding tokens. This is done by using an attention mask.\n",
    "\n",
    "## Attention masks\n",
    "Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model).\n",
    "\n",
    "Let’s complete the previous example with an attention mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7fd9e6fe-ed38-4845-ac70-027a710be924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '0' to ignore padding token id\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "attention_mask = torch.tensor(attention_mask, device=device)\n",
    "batched_ids = torch.tensor(batched_ids, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77fbc9a7-f47e-40db-bcb7-a04543d6a29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    }
   ],
   "source": [
    "outputs = model(batched_ids, attention_mask)\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "15bc7278-ca49-41c5-a985-3359787dfa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model(sequence1_ids).logits)\n",
    "print(model(sequence2_ids).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759254fe-2dc7-43ff-8204-2851ac3e9187",
   "metadata": {},
   "source": [
    "Now we get the same logits for the second sentence in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdd6b23-ad4b-4dd4-8a75-3ae0af890cff",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "```\n",
    "Apply the tokenization manually on the two sentences used in section 2 (“I’ve been waiting for a HuggingFace course my whole life.” and “I hate this so much!”). Pass them through the model and check that you get the same logits as in section 2. Now batch them together using the padding token, then create the proper attention mask. Check that you obtain the same results when going through the model!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84658371-ea54-4d77-a00a-48f52a4bf3fe",
   "metadata": {},
   "source": [
    "### Auto Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "6836b408-12a1-4015-92f9-70baf313fed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5607,  1.6123],\n",
       "        [ 4.1692, -3.3464]], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "     \"I hate this so much!\",\n",
    "]\n",
    "\n",
    "checkpoint = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "model_0 = AutoModelForSequenceClassification.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "token_results = tokenizer(\n",
    "    raw_inputs, padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "token_results.to(device)\n",
    "\n",
    "output = model_0(token_results.input_ids, token_results.attention_mask)\n",
    "output\n",
    "\n",
    "logits_1 = output.logits\n",
    "logits_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d99c0b-5758-42fe-a6ee-008733dcc24b",
   "metadata": {},
   "source": [
    "### Manual Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "f1b6c932-2e0c-48b7-aade-c47762b1ff4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5979,  1.6390],\n",
       "        [ 4.1692, -3.3464]], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "sentence_1 = \"I’ve been waiting for a HuggingFace course my whole life.\"\n",
    "sentence_2 = \"I hate this so much!\"\n",
    "\n",
    "checkpoint = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "model_1 = AutoModelForSequenceClassification.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Tokens\n",
    "token_sent_1 = tokenizer.tokenize(sentence_1)\n",
    "input_id_1 = tokenizer.convert_tokens_to_ids(token_sent_1)\n",
    "input_id_1 = tokenizer.prepare_for_model(input_id_1)\n",
    "input_id_1 = input_id_1['input_ids']\n",
    "# pprint(input_id_1, compact=True)\n",
    "# print(len(input_id_1))\n",
    "\n",
    "token_sent_2 = tokenizer.tokenize(sentence_2)\n",
    "input_id_2 = tokenizer.convert_tokens_to_ids(token_sent_2)\n",
    "input_id_2 = tokenizer.prepare_for_model(input_id_2)\n",
    "input_id_2 = input_id_2['input_ids']\n",
    "# pprint(input_id_2, compact=True)\n",
    "# print(len(input_id_2))\n",
    "\n",
    "padding_range = len(input_id_1) - len(input_id_2)\n",
    "# padding_range\n",
    "\n",
    "# Applying manual padding to input_id_2\n",
    "for pad in range(padding_range):\n",
    "    input_id_2.append(tokenizer.pad_token_id)\n",
    "\n",
    "# input_id_2\n",
    "\n",
    "# batched\n",
    "batched_ids = [input_id_1, input_id_2]\n",
    "# pprint(batched_ids, compact=True)\n",
    "\n",
    "# to tensor and device\n",
    "batched_ids = torch.tensor(batched_ids, device=device)\n",
    "\n",
    "# Creating attention masks\n",
    "attention_mask_1 = []\n",
    "attention_mask_2 = []\n",
    "\n",
    "for a in input_id_1:\n",
    "    if a != 0:\n",
    "        attention_mask_1.append(1)\n",
    "    else:\n",
    "        attention_mask_1.append(0)\n",
    "\n",
    "for a in input_id_2:\n",
    "    if a != 0:\n",
    "        attention_mask_2.append(1)\n",
    "    else:\n",
    "        attention_mask_2.append(0)\n",
    "\n",
    "# print(attention_mask_1)\n",
    "# print(attention_mask_2)\n",
    "\n",
    "attention_mask = [attention_mask_1, attention_mask_2]\n",
    "attention_mask = torch.tensor(attention_mask, device=device)\n",
    "# attention_mask\n",
    "\n",
    "# Output logits\n",
    "logits_2 = model_1(batched_ids, attention_mask).logits\n",
    "logits_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "3f38da24-9717-4f46-b3a2-8cb583102042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5607,  1.6123],\n",
       "        [ 4.1692, -3.3464]], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "86beb0c6-ec97-48a5-8540-84b50f2d15c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
       "          2607,  2026,  2878,  2166,  1012,   102],\n",
       "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]], device='mps:0')"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_results['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "9f6f708e-7fe8-4ffd-a862-a2b0dc176fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], device='mps:0')"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_results['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "910a96c1-acaf-440e-894a-40b5c9053359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1045,  1521,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
       "          2607,  2026,  2878,  2166,  1012,   102],\n",
       "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]], device='mps:0')"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "97920083-01fb-4dcb-b38c-8565c3fecf2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True], device='mps:0')"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_results['input_ids'][0] == batched_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "996fe6c0-7dca-43ed-9614-3a385fa6f4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True], device='mps:0')"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_results['input_ids'][1] == batched_ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "89917e21-e717-4ea2-aded-02134e9ac233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True], device='mps:0')"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_results['attention_mask'][0] == attention_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d55acef1-c79a-415a-a249-de70f8be87bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True], device='mps:0')"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_results['attention_mask'][1] == attention_mask[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b988c-a664-4813-b4f2-81a853138877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "434cb2ee-2b73-4b3b-be48-95ecff7b17a6",
   "metadata": {},
   "source": [
    "## Longer sequences\n",
    "With Transformer models, there is a limit to the lengths of the sequences we can pass the models. Most models handle sequences of up to 512 or 1024 tokens, and will crash when asked to process longer sequences. There are two solutions to this problem:\n",
    "\n",
    "- Use a model with a longer supported sequence length.\n",
    "- Truncate your sequences.\n",
    "  \n",
    "Models have different supported sequence lengths, and some specialize in handling very long sequences. \n",
    "\n",
    "`Longformer` is one example, and another is `LED`. If you’re working on a task that requires very long sequences, we recommend you take a look at those models.\n",
    "\n",
    "Otherwise, we recommend you truncate your sequences by specifying the max_sequence_length parameter:\n",
    "\n",
    "`sequence = sequence[:max_sequence_length]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "59c5eded-3548-451e-b7f2-7b76b4bc236e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0%3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "3fbd93d9-ffbe-4f7f-97ed-7baf31a78d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player choice: 0\n",
      "PrizeBox: 2\n",
      "By the way, box 1 is empty.\n",
      "Would you like to change your guess?\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "player_choice = 0\n",
    "print(f\"Player choice: {player_choice}\")\n",
    "# num = -1\n",
    "revealed = 0\n",
    "prize_box = random.randrange(3)\n",
    "print(f\"PrizeBox: {prize_box}\")\n",
    "\n",
    "def open_box():\n",
    "    # Reveals a box that is not the one selected by the user\n",
    "    #\tand also does not contain a prize\n",
    "    global revealed\n",
    "    # Assigns num to be either -1 or +1\n",
    "    # Num is a random direction starting from player_choice\n",
    "    num = random.randrange(-1, 3, 2)\n",
    "    revealed = (player_choice + num) % 3\n",
    "    if revealed == prize_box:\n",
    "        revealed = (revealed + num) % 3\n",
    "    print (\"By the way, box\", revealed, \"is empty.\")\n",
    "    print (\"Would you like to change your guess?\")\n",
    "\n",
    "open_box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "49c7b053-bb78-478d-9da5-60c376efd498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1%4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb167d5-46ee-47d0-875a-92b8a5d0cb69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
