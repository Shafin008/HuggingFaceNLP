{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d3d946-571f-4d1b-bed5-a609eb7e2995",
   "metadata": {},
   "source": [
    "## Train a sequence classifier \n",
    "Here is how we would train a sequence classifier on one batch in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd3a54da-d92c-4154-8168-4c519c01477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "205147ca-fd95-47c4-80fc-d6d668ce4119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "checkpoint = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af1daa46-690b-4c43-b26e-52407deccc48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e451e81a96084c11856213892cbcf3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ce8018043e45bb90e582136ef9edd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381890de56c340759bc8a337901fd845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9cfc285-57fd-4668-8c80-9644508e5e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# initialize model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdb2622a-6df9-4b6e-a4ac-9354da00749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0e074b6-d3b9-462a-9493-5665f201a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a batch\n",
    "batch = tokenizer(\n",
    "    sequences,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74504c12-58fd-46da-b752-37b57b518892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  2023,  2607,  2003,  6429,   999,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
      " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print(type(batch))\n",
    "pprint(batch, compact=True,sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83144dd7-8edb-46cd-9761-9895069efdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  2023,  2607,  2003,  6429,   999,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
      " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
      " 'labels': tensor([1, 1])}\n"
     ]
    }
   ],
   "source": [
    "# creating labels\n",
    "batch['labels'] = torch.tensor([1,1])\n",
    "pprint(batch, compact=True,sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d574e16-0b05-40c8-870d-eaa04ca46ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x13231aea0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99d170a2-ef0d-45eb-b892-e29a2c3ccb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shafinsaapel/tensorflow-test/env/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    betas: (0.9, 0.999)\n",
       "    correct_bias: True\n",
       "    eps: 1e-06\n",
       "    lr: 0.001\n",
       "    weight_decay: 0.0\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initiating optimizer\n",
    "optimizer = AdamW(model.parameters())\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43315305-90c3-48db-ba2d-e5ea956cb264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.7365, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1779, -0.2608],\n",
       "        [-0.2002, -0.2873]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output\n",
    "output = model(**batch)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e0ea763-12f0-415e-aee3-2d034b70d111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7365, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss\n",
    "loss = output.loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8031bf7-b096-49b4-a48e-f106e3f2229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backprop\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29de6fbe-c23c-4aaa-8b99-f86cad91bd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer step\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21bc8f0-8136-4724-94d7-07b0b30feb77",
   "metadata": {},
   "source": [
    "Training the model on two sentences is not going to yield very good results. To get better results, you will need to prepare a bigger dataset.\n",
    "\n",
    "In this section we will use as an example the MRPC (Microsoft Research Paraphrase Corpus) dataset, introduced in a paper by William B. Dolan and Chris Brockett. \n",
    "\n",
    "The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing). \n",
    "\n",
    "It’s a small dataset, so it’s easy to experiment with training on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4241b493-d1b7-4c31-947b-573e63fe6d4d",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "The HF Hub contains multiple datasets in lots of different languages. You can browse the datasets [here](https://huggingface.co/datasets).\n",
    "\n",
    "Let’s focus on the MRPC dataset! This is one of the 10 datasets composing the GLUE benchmark, which is an academic benchmark that is used to measure the performance of ML models across 10 different text classification tasks.\n",
    "\n",
    "The Datasets library provides a very simple command to download and cache a dataset on the Hub. We can download the MRPC dataset like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b496808-5cda-4cc2-a97f-a65ab4c3c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2249716d-b233-402c-b8db-7963b4d76f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62925ed5-ec82-43bc-9c9c-8895e0e2dfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdb2f5a-f53a-4d24-821c-ad4f85d9a1e8",
   "metadata": {},
   "source": [
    "As you can see, we get a DatasetDict object which contains the training set, the validation set, and the test set. Each of those contains several columns (sentence1, sentence2, label, and idx) and a variable number of rows, which are the number of elements in each set (so, there are 3,668 pairs of sentences in the training set, 408 in the validation set, and 1,725 in the test set).\n",
    "\n",
    "This command downloads and caches the dataset, by default in ~/.cache/huggingface/datasets. You can customize your cache folder by setting the HF_HOME environment variable.\n",
    "\n",
    "We can access each pair of sentences in our raw_datasets object by indexing, like with a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97a3978c-b5c2-4ffa-84fc-0b193a0c9471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "    num_rows: 3668\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets['train']\n",
    "raw_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a7b8c07-2ed3-48c3-999b-96370ef53610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7633f73a-89d8-4dff-8c7e-96ae92e3c590",
   "metadata": {},
   "source": [
    "We can see the labels are already integers, so we won’t have to do any preprocessing there. \n",
    "\n",
    "To know which integer corresponds to which label, we can inspect the features of our raw_train_dataset. \n",
    "\n",
    "This will tell us the type of each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82187aba-49e6-4ccf-bb79-078de8998694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84653efc-9f21-4829-a244-1a59f411a833",
   "metadata": {},
   "source": [
    "Behind the scenes, label is of type ClassLabel, and the mapping of integers to label name is stored in the names folder. 0 corresponds to `not_equivalent`, and 1 corresponds to `equivalent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e883d19-ac65-488c-848f-ceb1325278fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Gyorgy Heizler , head of the local disaster unit , said the coach was carrying 38 passengers .',\n",
       " 'sentence2': 'The head of the local disaster unit , Gyorgy Heizler , said the coach driver had failed to heed red stop lights .',\n",
       " 'label': 0,\n",
       " 'idx': 15}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c889184-f2fc-40a5-8545-4a0739a12515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "    num_rows: 408\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_valid_dataset = raw_datasets['validation']\n",
    "raw_valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8aef1e09-f6b3-4541-8951-55dc457768ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'He was arrested Friday night at an Alpharetta seafood restaurant while dining with his wife , singer Whitney Houston .',\n",
       " 'sentence2': 'He was arrested again Friday night at an Alpharetta restaurant where he was having dinner with his wife .',\n",
       " 'label': 1,\n",
       " 'idx': 796}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_valid_dataset[86]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "921d82b8-928f-4918-96a4-54eda258400e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': ['Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       "  \"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\",\n",
       "  'They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .',\n",
       "  'Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',\n",
       "  'The stock rose $ 2.11 , or about 11 percent , to close Friday at $ 21.51 on the New York Stock Exchange .'],\n",
       " 'sentence2': ['Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       "  \"Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\",\n",
       "  \"On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .\",\n",
       "  'Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .',\n",
       "  'PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .'],\n",
       " 'label': [1, 0, 1, 0, 1],\n",
       " 'idx': [0, 1, 2, 3, 4]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b28889c-9615-44e3-98b5-8662ee0edba7",
   "metadata": {},
   "source": [
    "To preprocess the dataset, we need to convert the text to numbers the model can make sense of which is done with a tokenizer. \n",
    "\n",
    "We can feed the tokenizer one sentence or a list of sentences, so we can directly tokenize all the first sentences and all the second sentences of each pair like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "822a923c-f00c-4c65-bc18-3f6559e66b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63c5d1eb-1e5b-4f3a-adf3-fa568ac67f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "faa80439-7f74-46ed-b8e2-a40474db258a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "    num_rows: 3668\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54fd574d-82ef-498d-b453-56b03d729280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3668, 3668)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_train_dataset['sentence1']), len(raw_train_dataset['sentence2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "31f77cf5-36f1-4d80-861a-47a704bc0d66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_sentences_1 = tokenizer(\n",
    "    raw_train_dataset[\"sentence1\"]\n",
    ")\n",
    "tokenized_sentences_2 = tokenizer(\n",
    "    raw_train_dataset[\"sentence2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb819282-28dd-4fb5-879c-a593a627d18e",
   "metadata": {},
   "source": [
    "However, we can’t just pass two sequences to the model and get a prediction of whether the two sentences are paraphrases or not. \n",
    "\n",
    "We need to handle the two sequences as a pair, and apply the appropriate preprocessing. \n",
    "\n",
    "Fortunately, the tokenizer can also take a pair of sequences and prepare it the way our BERT model expects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "92ecd177-fb9f-463b-b2fe-545c56772023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example sequence\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "47b0408b-235f-415c-bf3f-06cb8a978d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(\n",
    "    sequences,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "87ed3012-6875-4a4c-be1e-36ba1b9442f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,   146,   112,  1396,  1151,  2613,  1111,   170, 20164, 10932,\n",
      "          2271,  7954,  1736,  1139,  2006,  1297,   119,   102],\n",
      "        [  101,  1188,  1736,  1110,  6929,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
      " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "pprint(batch, compact=True, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "7713f153-e8a7-46aa-adfd-17eab69ac3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1422, 1271, 1110, 156, 7777, 2497, 1394, 119, 102, 146,\n",
      "               1250, 1120, 20164, 10932, 2271, 7954, 119, 102],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# exmaple of tokenizer taking pairs of sentences\n",
    "pprint(tokenizer(\n",
    "    \"My name is Sylvain.\",\n",
    "    \"I work at HuggingFace.\"\n",
    "), compact=True, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "db36cb06-f054-453e-ac44-75217af8a881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1422, 1271, 1110, 156, 7777, 2497, 1394, 119, 102, 146,\n",
      "                1250, 1120, 20164, 10932, 2271, 7954, 119, 102],\n",
      "               [101, 11099, 1106, 7678, 119, 102, 1188, 2523, 1110, 1632, 119,\n",
      "                102]],\n",
      " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]],\n",
      " 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# Example of multi pair sentences\n",
    "pprint(tokenizer(\n",
    "    [\"My name is Sylvain.\", \"Going to cinema.\"],\n",
    "    [\"I work at HuggingFace.\", \"This movie is great.\"]\n",
    "), compact=True, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5efe0bf-248e-4481-9c2d-19d694a18488",
   "metadata": {},
   "source": [
    "`token_type_ids` is what tells the model which part of the input is the first sentence and which is the second sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43078cbb-2466-4830-8d5f-1d167e2a26b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee10cd43-cf83-4f62-b18b-899efea40a51",
   "metadata": {},
   "source": [
    "## Try it out!\n",
    "\n",
    "Take element 15 of the training set and tokenize the two sentences separately and as a pair. What’s the difference between the two results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e27982e-4e53-4c72-8fd9-241e6b06fe12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Gyorgy Heizler , head of the local disaster unit , said the coach was carrying 38 passengers .',\n",
       " 'sentence2': 'The head of the local disaster unit , Gyorgy Heizler , said the coach driver had failed to heed red stop lights .',\n",
       " 'label': 0,\n",
       " 'idx': 15}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset[14]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090f689-4ef1-4147-a294-1960540053c4",
   "metadata": {},
   "source": [
    "### Separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e15325d4-853a-4fc4-b376-6a07a6d46993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gyorgy Heizler , head of the local disaster unit , said the coach was carrying 38 passengers .'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_1 = raw_train_dataset[14]['sentence1']\n",
    "sent_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "415a561e-764f-46d7-b5c4-fd1457fa91dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The head of the local disaster unit , Gyorgy Heizler , said the coach driver had failed to heed red stop lights .'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_2 = raw_train_dataset[14]['sentence2']\n",
    "sent_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "94b9a1f0-9e54-4c6a-9176-cc9074d094e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4fabf9a5-3820-4191-a249-0f59b6f7a741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1043, 7677, 22637, 2002, 10993, 3917, 1010, 2132, 1997,\n",
      "               1996, 2334, 7071, 3131, 1010, 2056, 1996, 2873, 2001, 4755, 4229,\n",
      "               5467, 1012, 102],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0, 0, 0, 0],\n",
      " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "sent1_tok = tokenizer(sent_1)\n",
    "pprint(sent1_tok, compact=True,sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5cc0951a-8c04-42c9-bd8d-cb9a194a979e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent1_tok['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e8c0cba3-249b-49bf-affc-bcdb4b8caa03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1996, 2132, 1997, 1996, 2334, 7071, 3131, 1010, 1043, 7677,\n",
      "               22637, 2002, 10993, 3917, 1010, 2056, 1996, 2873, 4062, 2018,\n",
      "               3478, 2000, 18235, 2094, 2417, 2644, 4597, 1012, 102],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "sent2_tok = tokenizer(sent_2)\n",
    "pprint(sent2_tok, compact=True,sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "32681022-30e4-4609-b3c3-caa667e88d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent2_tok['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a806138-59ed-44ac-8f0e-3fa47e9072f8",
   "metadata": {},
   "source": [
    "### As a pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f4288bac-15e1-4429-ad77-399c26ad6bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1043, 7677, 22637, 2002, 10993, 3917, 1010, 2132, 1997,\n",
      "               1996, 2334, 7071, 3131, 1010, 2056, 1996, 2873, 2001, 4755, 4229,\n",
      "               5467, 1012, 102, 1996, 2132, 1997, 1996, 2334, 7071, 3131, 1010,\n",
      "               1043, 7677, 22637, 2002, 10993, 3917, 1010, 2056, 1996, 2873,\n",
      "               4062, 2018, 3478, 2000, 18235, 2094, 2417, 2644, 4597, 1012,\n",
      "               102],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "paired_sent_tok = tokenizer(\n",
    "    sent_1,\n",
    "    sent_2\n",
    ")\n",
    "pprint(paired_sent_tok, compact=True,sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "724cb437-b5b0-4bbc-ae7c-57bd76ddcd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paired_sent_tok['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10134aa3-04e6-4162-8196-153cd059ac7a",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "058f9592-632e-4825-8f6b-a59ef6d26ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996,\n",
      "               2117, 2028, 1012, 102],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# sending the tokenizer two sequences at once\n",
    "inputs = tokenizer(\n",
    "    \"This is the first sentence.\", \n",
    "    \"This is the second one.\"\n",
    ")\n",
    "pprint(inputs, compact=True, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8e713987-d677-4968-a708-716acf1d5ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is',\n",
      " 'the', 'second', 'one', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# first we need to convert ids to tokens\n",
    "pprint(tokenizer.convert_ids_to_tokens(inputs['input_ids']), compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb63f7ee-4778-4e82-9e42-0a419e6881ae",
   "metadata": {},
   "source": [
    "So we see the model expects the inputs to be of the form [CLS] sentence1 [SEP] sentence2 [SEP] when there are two sentences. \n",
    "\n",
    "Aligning this with the `token_type_ids` gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0e966430-7e5a-4151-8a3b-9c6d9c4611cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is',\n",
      " 'the', 'second', 'one', '.', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.convert_ids_to_tokens(inputs['input_ids']), compact=True)\n",
    "pprint(inputs['token_type_ids'], compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb30143-2f43-4883-903d-998151b476b4",
   "metadata": {},
   "source": [
    "As you can see, the parts of the input corresponding to [CLS] sentence1 [SEP] all have a token type ID of 0, while the other parts, corresponding to sentence2 [SEP], all have a token type ID of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4adfd4-07c9-4ef8-afc0-d19cf3095ae3",
   "metadata": {},
   "source": [
    "Note that if you select a different checkpoint, you won’t necessarily have the token_type_ids in your tokenized inputs (for instance, they’re not returned if you use a DistilBERT model). \n",
    "\n",
    "They are only returned when the model will know what to do with them, because it has seen them during its pretraining.\n",
    "\n",
    "Here, BERT is pretrained with token type IDs, and on top of the masked language modeling objective, it has an additional objective called next sentence prediction. The goal with this task is to model the relationship between pairs of sentences.\n",
    "\n",
    "With next sentence prediction, the model is provided pairs of sentences (with randomly masked tokens) and asked to predict whether the second sentence follows the first. To make the task non-trivial, half of the time the sentences follow each other in the original document they were extracted from, and the other half of the time the two sentences come from two different documents.\n",
    "\n",
    "In general, you don’t need to worry about whether or not there are token_type_ids in your tokenized inputs: as long as you use the same checkpoint for the tokenizer and the model, everything will be fine as the tokenizer knows what to provide to its model.\n",
    "\n",
    "\n",
    "## Fixed Padding\n",
    "\n",
    "To keep the data as a dataset, we will use the Dataset.map() method. This also allows us some extra flexibility, if we need more preprocessing done than just tokenization. The map() method works by applying a function on each element of the dataset. \n",
    "\n",
    "Let’s define a function that tokenizes our inputs. This function takes a dictionary (like the items of our dataset) and returns a new dictionary with the keys input_ids, attention_mask, and token_type_ids. \n",
    "\n",
    "Now that we have seen how our tokenizer can deal with one pair of sentences, we can use it to tokenize our whole dataset, we can feed the tokenizer a list of pairs of sentences by giving it the list of first sentences, then the list of second sentences. This is also compatible with the padding and truncation options. So, one way to preprocess the training dataset is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c4e74a3b-99c4-4f83-817f-4766ae4a7238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7c34d05e-3d07-4372-b8a3-c0d7b79630f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed padding example\n",
    "raw_dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0bcdc62b-8070-4088-ab3c-86cea36a20dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e3479144-1174-4611-b243-6751a9003f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"sentence1\"],\n",
    "        example[\"sentence2\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b90b268c-9561-4a87-ae7f-75ba54e5ad62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a0094b62164993a681bb32624d133b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## To speed up we can use batch\n",
    "tokenized_datasets = raw_dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b026bbc0-2599-4586-8f43-78b8ddf59db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': ['sentence1', 'sentence2', 'label', 'idx', 'input_ids',\n",
      "          'token_type_ids', 'attention_mask'],\n",
      " 'train': ['sentence1', 'sentence2', 'label', 'idx', 'input_ids',\n",
      "           'token_type_ids', 'attention_mask'],\n",
      " 'validation': ['sentence1', 'sentence2', 'label', 'idx', 'input_ids',\n",
      "                'token_type_ids', 'attention_mask']}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(tokenized_datasets.column_names, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "aa89a1a1-cecc-4525-9681-825cb0502725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare for training\n",
    "\n",
    "# Once we are done with tokenization, \n",
    "# we don't need idx, sentence1, sentence2 columns\n",
    "# removing the columns with idx, sentence1, sentence2\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    [\"idx\", \"sentence1\", \"sentence2\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fd56cc54-067e-4e1a-8545-d4cefce2b0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## renaming the label column to labels\n",
    "# as huggingface expects it like this\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\n",
    "    \"label\", \"labels\"\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "049c013a-64dc-4d27-b989-c755083186e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## finally selecting the output tensor format\n",
    "tokenized_datasets = tokenized_datasets.with_format(\n",
    "    \"torch\"\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6ab353bb-dbc6-4096-ac9c-b546d489ed17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3668"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_datasets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c16da5ef-674c-4042-a1c1-e1a592a501d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can also create a sample dataset\n",
    "## We'll create a sample dataset from the training dataset with 100 samples\n",
    "small_train_dataset = tokenized_datasets[\"train\"].select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "53accac3-2a8c-4dd9-a9f7-8a30056a2f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a1d9db-0eb0-4dc2-b314-da36b6fb450f",
   "metadata": {},
   "source": [
    "This works well, but it has the disadvantage of returning a dictionary (with our keys, input_ids, attention_mask, and token_type_ids, and values that are lists of lists). \n",
    "\n",
    "It will also only work if you have enough RAM to store your whole dataset during the tokenization (whereas the datasets from the Datasets library are Apache Arrow files stored on the disk, so you only keep the samples you ask for loaded in memory).\n",
    "\n",
    "\n",
    "\n",
    "Note that it also works if the example dictionary contains several samples (each key as a list of sentences) since the tokenizer works on lists of pairs of sentences, as seen before. \n",
    "\n",
    "This will allow us to use the option batched=True in our call to map(), which will greatly speed up the tokenization. \n",
    "\n",
    "Here is how we apply the tokenization function on all our datasets at once. We’re using batched=True in our call to map so the function is applied to multiple elements of our dataset at once, and not on each element separately. This allows for faster preprocessing.\n",
    "\n",
    "The tokenizer is backed by a tokenizer written in Rust from the 🤗 Tokenizers library. This tokenizer can be very fast, but only if we give it lots of inputs at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6cde92ea-b973-4d6e-ad4b-8337763bb970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed padding example\n",
    "raw_dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"sentence1\"],\n",
    "        example[\"sentence2\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "tokenized_datasets = raw_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    [\"idx\", \"sentence1\", \"sentence2\"]\n",
    ")\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\n",
    "    \"label\", \"labels\"\n",
    ")\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.with_format(\n",
    "    \"torch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c57137b3-c1cc-4157-a335-a2e084c380c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing this to a pytorch dataloader\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8823d428-2054-4f31-add0-d2cffcf0d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "71717395-9d03-41ca-a03d-7416bb9bd5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    if step > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74750811-fbc3-4e71-a1e2-ffecf7312749",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "`torch.Size([batch_size, max_length])` fixed batch size of 16 and fixed max length of 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f409b3-81de-4d26-8b91-40f00a92f138",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e959469-d26b-4b32-84a0-46e5a1f5b8b8",
   "metadata": {},
   "source": [
    "## Dynamic Padding\n",
    "\n",
    "The function that is responsible for putting together samples inside a batch is called a collate function. It’s an argument you can pass when you build a DataLoader, the default being a function that will just convert your samples to PyTorch tensors and concatenate them (recursively if your elements are lists, tuples, or dictionaries). \n",
    "\n",
    "This won’t be possible in our case since the inputs we have won’t all be of the same size. We have deliberately postponed the padding, to only apply it as necessary on each batch and avoid having over-long inputs with a lot of padding. This will speed up training by quite a bit, but note that if you’re training on a TPU it can cause problems — TPUs prefer fixed shapes, even when that requires extra padding.\n",
    "\n",
    "To do this in practice, we have to define a collate function that will apply the correct amount of padding to the items of the dataset we want to batch together. \n",
    "\n",
    "Fortunately, the Transformers library provides us with such a function via DataCollatorWithPadding. It takes a tokenizer when you instantiate it (to know which padding token to use, and whether the model expects padding to be on the left or on the right of the inputs) and will do everything you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c1a34eac-50cf-4a1e-9d04-f0cf5eae2890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No fixed padding\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"sentence1\"],\n",
    "        example[\"sentence2\"],\n",
    "        truncation=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93beea0-0f55-4c02-b003-d7ab1aa1b337",
   "metadata": {},
   "source": [
    "Note that we’ve left the padding argument out in our tokenization function for now. This is because padding all the samples to the maximum length is not efficient: it’s better to pad the samples when we’re building a batch, as then we only need to pad to the maximum length in that batch, and not the maximum length in the entire dataset. This can save a lot of time and processing power when the inputs have very variable lengths!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "728944fb-8eac-4348-a803-3950ae5df3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37658cfdc7364d089c2ebaaf0aaefc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce23239b-b440-4871-a255-ce4721d61060",
   "metadata": {},
   "source": [
    "The way the Datasets library applies this processing is by adding new fields to the datasets, one for each key in the dictionary returned by the preprocessing function.\n",
    "\n",
    "You can even use multiprocessing when applying your preprocessing function with map() by passing along a num_proc argument. We didn’t do this here because the Tokenizers library already uses multiple threads to tokenize our samples faster, but if you are not using a fast tokenizer backed by this library, this could speed up your preprocessing.\n",
    "\n",
    "Our tokenize_function returns a dictionary with the keys input_ids, attention_mask, and token_type_ids, so those three fields are added to all splits of our dataset.\n",
    "\n",
    "Note that we could also have changed existing fields if our preprocessing function returned a new value for an existing key in the dataset to which we applied map().\n",
    "\n",
    "The last thing we will need to do is pad all the examples to the length of the longest element when we batch elements together — a technique we refer to as dynamic padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1929d04d-d1e8-4144-8716-9936e66b8e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ed162957-d2c8-4372-aba7-9624e5dfa535",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b9b38dc0-0f05-49fa-b773-5a8d0018ed09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorWithPadding(tokenizer=BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       "), padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94de9aaa-a74b-4ec1-aa94-262b3246fbae",
   "metadata": {},
   "source": [
    "let’s grab a few samples from our training set that we would like to batch together. \n",
    "\n",
    "Here, we remove the columns idx, sentence1, and sentence2 as they won’t be needed and contain strings (and we can’t create tensors with strings) and have a look at the lengths of each entry in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "207b4e09-ad32-43d6-b935-88b0d279cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    [\"idx\", \"sentence1\", \"sentence2\"]\n",
    ")\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\n",
    "    \"label\", \"labels\"\n",
    ")\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.with_format(\n",
    "    \"torch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "44bd7b7a-b806-4073-a86f-df85d1071ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "5547f8c0-f4a6-46ac-992b-aa77e406d0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 72])\n",
      "torch.Size([16, 67])\n",
      "torch.Size([16, 67])\n",
      "torch.Size([16, 78])\n",
      "torch.Size([16, 86])\n",
      "torch.Size([16, 74])\n",
      "torch.Size([16, 72])\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    if step > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b704aff-03cc-4af0-8948-089849251f15",
   "metadata": {},
   "source": [
    "we can see fixed batch size of 16 but different max length in different batches. This will speed up the process much faster in CPU and GPU but not in TPU. TPUs need fixed padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b719449-bf96-47e2-a107-1244ea2a9901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3514c3e-411e-494c-8f7f-762e99ba9e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a1bfa5-8f93-46a5-af36-0792305de101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
